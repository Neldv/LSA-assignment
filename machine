import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('/kaggle/input/machine-learning/our-project.csv')

# Define the feature columns and target column
feature_columns = ['visit_age', 'bmi', 'geo_mean', 'd_geo_mean']
target_column = 'vaccine_response'

# Convert categorical columns to numeric using one-hot encoding
data = pd.get_dummies(data, columns=['gender', 'race', 'cmv_status', 'ebv_status', 'statin_use'], drop_first=True)

# Drop rows where the target variable 'vaccine_response' is NaN (missing values)
data_cleaned = data.dropna(subset=[target_column])

# Fill missing values for numerical columns with the mean
for column in feature_columns:
    data_cleaned[column].fillna(data_cleaned[column].mean(), inplace=True)

# Fill missing values for categorical columns with the most frequent value
for column in data_cleaned.columns:
    if data_cleaned[column].dtype == 'object':
        data_cleaned[column].fillna(data_cleaned[column].mode()[0], inplace=True)

# Split the data into features (X) and target (y)
X = data_cleaned[feature_columns]
y = data_cleaned[target_column]
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)




### RANDOM FOREST MODEL
random_f = RandomForestClassifier(n_estimators=100, random_state=42)
random_f.fit(X_train, y_train)
rf_pred = random_f.predict(X_test)

# Evaluate the models
print("Random Forest Model Accuracy:", accuracy_score(y_test, rf_pred))
print("Random Forest Model Classification Report:\n", classification_report(y_test, rf_pred))


# Calculate and print the evaluation metrics
mseRF = mean_squared_error(y_test, rf_pred)  # Mean Squared Error
maeRF = mean_absolute_error(y_test, rf_pred)  # Mean Absolute Error
r2RF = r2_score(y_test, rf_pred)  # R-squared score

print(f"Mean Squared Error RF: {mseRF:.2f}")
print(f"Mean Absolute Error RF: {maeRF:.2f}")
print(f"R-squared RF: {r2RF:.2f}")

# Generate and plot the residuals (difference between actual and predicted values)
residualsrf = y_test - rf_pred

plt.figure(figsize=(8, 6))
sns.histplot(residualsrf, kde=True, color='blue', bins=30)
plt.title("Residuals of Random forest model")
plt.xlabel("Residuals")
plt.ylabel("Frequency")
plt.tight_layout()
plt.show()

# Confusion Matrix for Random Forest Model
conf_matrix_rf = confusion_matrix(y_test, rf_pred)
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix_rf, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix for Random Forest')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()




## LOGISTIC REGRESSION

import pandas as pd  # Import pandas for data manipulation
from sklearn.model_selection import train_test_split  # Import function to split data
from sklearn.linear_model import LogisticRegression  # Import Logistic Regression model
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report  # Import evaluation metrics
from sklearn.preprocessing import LabelEncoder  # Import LabelEncoder for encoding categorical features
import seaborn as sns  # Import seaborn for visualization
import matplotlib.pyplot as plt  # Import matplotlib for plotting

# Load your dataset (replace with your actual path)
data = pd.read_csv('../input/machine-learning/our-project.csv')

# Define target and features
target_column_name = 'vaccine_response'  # Define the target variable (the column we want to predict)
features = ['vaccine', 'race', 'cmv_status','gender','ebv_status', 'd_geo_mean', 'visit_age','geo_mean' ,'total_vaccines_received', 'bmi',  'vaccinated_2yr_prior', 'vaccinated_1yr_prior']  # Define the features we will use for prediction

# Drop rows where the target variable 'vaccine_response' is NaN (missing values)
data_cleaned = data.dropna(subset=[target_column_name])  # Remove rows with missing target values

# Prepare the features (X) and target (y)
X = data_cleaned[features].copy()  # Create a copy of the DataFrame to avoid SettingWithCopyWarning
y = data_cleaned[target_column_name]  # Extract the target variable (output)

# Encode categorical columns safely using LabelEncoder
le = LabelEncoder()
X['vaccine'] = le.fit_transform(X['vaccine'])  # Safely convert 'vaccine' to numeric values
X['race'] = le.fit_transform(X['race'])        # Safely convert 'race' to numeric values
X['cmv_status'] = le.fit_transform(X['cmv_status'])  # Safely convert 'cmv_status'
X['ebv_status'] = le.fit_transform(X['ebv_status'])  # Safely convert 'ebv_status'
X['d_geo_mean'] = le.fit_transform(X['d_geo_mean'])  # Safely convert 'd_geo_mean'
X['geo_mean'] = le.fit_transform(X['geo_mean'])  # Safely convert 'geo_mean'
X['bmi'] = le.fit_transform(X['bmi'])  # Safely convert 'bmi'
X['total_vaccines_received'] = le.fit_transform(X['total_vaccines_received'])  # Safely convert 'total_vaccines_received'
X['vaccinated_1yr_prior'] = le.fit_transform(X['vaccinated_1yr_prior'])  # Safely convert 'vaccinated_1yr_prior'
X['vaccinated_2yr_prior'] = le.fit_transform(X['vaccinated_2yr_prior'])  # Safely convert 'vaccinated_2yr_prior'
X['gender'] = le.fit_transform(X['gender'])  # Safely convert 'gender'

# Split the data into training and validation sets (80% training, 20% validation)
train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Logistic Regression model (suitable for classification tasks)
logreg_model = LogisticRegression(max_iter=1000)  # Initialize Logistic Regression model
logreg_model.fit(train_X, train_y)  # Train the model using training data

# Make predictions on the validation set
logreg_predictions = logreg_model.predict(val_X)  # Predict the target variable for the validation set

# Calculate and print the evaluation metrics
accuracy = accuracy_score(val_y, logreg_predictions)  # Accuracy
conf_matrix = confusion_matrix(val_y, logreg_predictions)  # Confusion Matrix
class_report = classification_report(val_y, logreg_predictions)  # Classification Report

print(f"Accuracy: {accuracy:.2f}")
print("Confusion Matrix:")
print(conf_matrix)
print("Classification Report:")
print(class_report)

# Feature Contributions (coefficients for Logistic Regression)
feature_contributions = pd.DataFrame({
    'Feature': train_X.columns,
    'Coefficient': logreg_model.coef_[0]
}).sort_values(by='Coefficient', ascending=False)

print("\nFeature Contributions (Logistic Regression Coefficients):")
print(feature_contributions)

# Visualizing the feature contributions
plt.figure(figsize=(10, 6))
sns.barplot(x=feature_contributions['Coefficient'], y=feature_contributions['Feature'], palette='viridis')
plt.title('Feature Contributions (Logistic Regression Coefficients)')
plt.xlabel('Coefficient Value')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix for Logistic Regression')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()


## Decision tree

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt


# Decision Tree Model
decision_tree = DecisionTreeClassifier(random_state=42)
decision_tree.fit(X_train, y_train)
dt_pred = decision_tree.predict(X_test)

# Evaluate the model
print("Decision Tree Model Accuracy:", accuracy_score(y_test, dt_pred))
print("Decision Tree Model Classification Report:\n", classification_report(y_test, dt_pred))

# Calculate and print the evaluation metrics
mse_dt = mean_squared_error(y_test, dt_pred)  # Mean Squared Error
mae_dt = mean_absolute_error(y_test, dt_pred)  # Mean Absolute Error
r2_dt = r2_score(y_test, dt_pred)  # R-squared score

print(f"Mean Squared Error RF: {mse_dt:.2f}")
print(f"Mean Absolute Error RF: {mae_dt:.2f}")
print(f"R-squared RF: {r2_dt:.2f}")

# Generate and plot the residuals (difference between actual and predicted values)
residuals_dt = y_test - dt_pred

plt.figure(figsize=(8, 6))
sns.histplot(residuals_dt, kde=True, color='blue', bins=30)
plt.title("Residuals of Decision Tree model")
plt.xlabel("Residuals")
plt.ylabel("Frequency")
plt.tight_layout()
plt.show()

# Confusion Matrix for Decision Tree Model
conf_matrix_dt_clf = confusion_matrix(y_test, dt_pred)
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix_dt_clf, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix for Decision Tree')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
