import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, mean_absolute_error, r2_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('/kaggle/input/helpme/our_project.csv')

# Define the initial feature columns and target column
initial_feature_columns = ['visit_age', 'bmi', 'geo_mean', 'd_geo_mean']
target_column = 'vaccine_response'

# Convert categorical columns to numeric using one-hot encoding
data = pd.get_dummies(data, columns=['gender', 'race', 'cmv_status', 'ebv_status', 'statin_use'], drop_first=True)

# Update the feature columns to include the new one-hot encoded columns
feature_columns = initial_feature_columns + [col for col in data.columns if col.startswith(('gender_', 'race_', 'cmv_status_', 'ebv_status_', 'statin_use_'))]

# Drop rows where the target variable 'vaccine_response' is NaN (missing values)
data_cleaned = data.dropna(subset=[target_column])

# Fill missing values for numerical columns with the mean
for column in initial_feature_columns:
    data_cleaned[column].fillna(data_cleaned[column].mean(), inplace=True)

# Fill missing values for categorical columns with the most frequent value
for column in data_cleaned.columns:
    if data_cleaned[column].dtype == 'object':
        data_cleaned[column].fillna(data_cleaned[column].mode()[0], inplace=True)

# Split the data into features (X) and target (y)
X = data_cleaned[feature_columns]
y = data_cleaned[target_column]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

### RANDOM FOREST MODEL
random_f = RandomForestClassifier(n_estimators=100, random_state=42)
random_f.fit(X_train, y_train)
rf_pred = random_f.predict(X_test)

# Evaluate the models
print("Random Forest Model Accuracy:", accuracy_score(y_test, rf_pred))
print("Random Forest Model Classification Report:\n", classification_report(y_test, rf_pred))

# Calculate and print the evaluation metrics
mseRF = mean_squared_error(y_test, rf_pred)  # Mean Squared Error
maeRF = mean_absolute_error(y_test, rf_pred)  # Mean Absolute Error
r2RF = r2_score(y_test, rf_pred)  # R-squared score

print(f"Mean Squared Error RF: {mseRF:.2f}")
print(f"Mean Absolute Error RF: {maeRF:.2f}")
print(f"R-squared RF: {r2RF:.2f}")

# Generate and plot the residuals (difference between actual and predicted values)
residualsrf = y_test - rf_pred

plt.figure(figsize=(8, 6))
sns.histplot(residualsrf, kde=True, color='blue', bins=30)
plt.title("Residuals of Random forest model")
plt.xlabel("Residuals")
plt.ylabel("Frequency")
plt.tight_layout()
plt.show()

# Confusion Matrix for Random Forest Model
conf_matrix_rf = confusion_matrix(y_test, rf_pred)
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix_rf, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix for Random Forest')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

## LOGISTIC REGRESSION
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train, y_train)
logr_pred = log_reg.predict(X_test)

# Evaluate the models
print("Logistic Regression Model Accuracy:", accuracy_score(y_test, logr_pred))
print("Logistic Regression Model Classification Report:\n", classification_report(y_test, logr_pred))

# Calculate and print the evaluation metrics
mse_lr = mean_squared_error(y_test, logr_pred)  # Mean Squared Error
mae_lr = mean_absolute_error(y_test, logr_pred)  # Mean Absolute Error
r2_lr = r2_score(y_test, logr_pred)  # R-squared score

print(f"Mean Squared Error LR: {mse_lr:.2f}")
print(f"Mean Absolute Error LR: {mae_lr:.2f}")
print(f"R-squared LR: {r2_lr:.2f}")

# Generate and plot the residuals (difference between actual and predicted values)
residualslg = y_test - logr_pred

plt.figure(figsize=(8, 6))
sns.histplot(residualslg, kde=True, color='blue', bins=30)
plt.title("Residuals of Logistic Regression Model")
plt.xlabel("Residuals")
plt.ylabel("Frequency")
plt.tight_layout()
plt.show()

# Confusion Matrix for Logistic Regression Model
conf_matrix_lr = confusion_matrix(y_test, logr_pred)
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix_lr, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix for Logistic Regression')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()


## Decision tree

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt


# Decision Tree Model
decision_tree = DecisionTreeClassifier(random_state=42)
decision_tree.fit(X_train, y_train)
dt_pred = decision_tree.predict(X_test)

# Evaluate the model
print("Decision Tree Model Accuracy:", accuracy_score(y_test, dt_pred))
print("Decision Tree Model Classification Report:\n", classification_report(y_test, dt_pred))

# Calculate and print the evaluation metrics
mse_dt = mean_squared_error(y_test, dt_pred)  # Mean Squared Error
mae_dt = mean_absolute_error(y_test, dt_pred)  # Mean Absolute Error
r2_dt = r2_score(y_test, dt_pred)  # R-squared score

print(f"Mean Squared Error RF: {mse_dt:.2f}")
print(f"Mean Absolute Error RF: {mae_dt:.2f}")
print(f"R-squared RF: {r2_dt:.2f}")

# Generate and plot the residuals (difference between actual and predicted values)
residuals_dt = y_test - dt_pred

plt.figure(figsize=(8, 6))
sns.histplot(residuals_dt, kde=True, color='blue', bins=30)
plt.title("Residuals of Decision Tree model")
plt.xlabel("Residuals")
plt.ylabel("Frequency")
plt.tight_layout()
plt.show()

# Confusion Matrix for Decision Tree Model
conf_matrix_dt_clf = confusion_matrix(y_test, dt_pred)
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix_dt_clf, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix for Decision Tree')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
